# Kubernetes RAG Implementation Guide with Weaviate and Ollama

## Solution Structure

The RAG (Retrieval-Augmented Generation) system for Kubernetes uses a local Weaviate instance with Ollama integration instead of OpenAI. The application automatically classifies user questions and routes them to appropriate data sources.

## Architecture

- **Weaviate**: Local vector database with embeddings generated by Ollama
- **Ollama**: Local embedding model (`nomic-embed-text`) and generative model (`llama3.1`)
- **Data Sources**: Markdown (theory) and CSV (practice/metrics)

## 1. Weaviate Database Configuration

```python
import weaviate
from weaviate.classes.config import Configure, Property, DataType

class KubernetesRAGSystem:
    def __init__(self, weaviate_url: str = "localhost", port: int = 8080):
        self.weaviate_url = weaviate_url
        self.port = port
        # Connection to local Weaviate instance
        self.client = weaviate.connect_to_local(
            host=self.weaviate_url, 
            port=self.port, 
            grpc_port=50052
        )
        self.collection_name = "PromQLKnowledge"

        if self.client.is_live():
            print("Weaviate server is active.")
        else:
            print("Weaviate server is not responding.")
```

### Creating Collection with Ollama Integration

```python
def setup_collection(self):
    """Creates a collection in Weaviate"""
    # Delete collection if exists
    try:
        self.client.collections.delete(self.collection_name)
        print(f"Deleted existing collection {self.collection_name}")
    except:
        pass
    
    # Create collection with Ollama configuration
    self.client.collections.create(
        name=self.collection_name,
        # Embeddings configuration via Ollama
        vector_config=Configure.Vectors.text2vec_ollama(
            api_endpoint="http://host.docker.internal:11434",
            model="nomic-embed-text",  # Embedding model
            source_properties=["text"],  # only this field will be embedded
            name="text"
        ),
        # Generative LLM configuration via Ollama
        generative_config=Configure.Generative.ollama(
            api_endpoint="http://host.docker.internal:11434",
            model="llama3.1"  # Generative model
        ),
        properties=[
            Property(name="text", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="source", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="chunk_type", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="category", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="section", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="subsection", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="query", data_type=DataType.TEXT, skip_vectorization=True),
            Property(name="metadata_json", data_type=DataType.TEXT, skip_vectorization=True),
        ]
    )
    print(f"Created collection {self.collection_name}")
```

## 2. Data Import (Batch Processing)

```python
def import_data(self, json_file_path: str):
    """Imports data from a JSON file to Weaviate using batch processing"""
    collection = self.client.collections.get(self.collection_name)
    
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        print(f"Loaded {len(data)} records from {json_file_path}")
    
    print("Starting data import to Weaviate...")
    
    # Using batch processing for performance
    with collection.batch.fixed_size(batch_size=100) as batch:
        imported_count = 0
        for item in data:
            weaviate_object = {
                "text": item.get("text", ""),
                "source": item.get("source", ""),
                "chunk_type": item.get("chunk_type", ""),
                "category": item.get("category", ""),
                "section": item.get("section", ""),
                "subsection": item.get("subsection") or "",  # None -> empty string
                "query": item.get("query") or "",  # None -> empty string
                "metadata_json": json.dumps(item.get("metadata", {}), ensure_ascii=False)
            }
            
            batch.add_object(properties=weaviate_object)
            imported_count += 1
            
            if imported_count % 10 == 0:
                print(f"Imported {imported_count}/{len(data)} objects...")
    
    # Check for import errors
    if len(collection.batch.failed_objects) > 0:
        print(f"Failed to import {len(collection.batch.failed_objects)} objects")

    self.client.close()  # Close client after import  
    print(f"Successfully imported {int(imported_count) - len(collection.batch.failed_objects)} objects!")
```

## 3. Question Classifier

```python
def classify_query_type(self, user_question: str) -> QueryType:
    """Classifies the user's question as general or about cluster status"""
    user_question_lower = user_question.lower()
    
    # Patterns for cluster status queries (specific metrics/monitoring)
    cluster_status_patterns = [
        r"ile\s+(podów|pod|nodes?|node|deployments?|svc|services?)",
        r"how many\s+(pods?|nodes?|deployments?|services?)",
        r"which\s+(pods?|nodes?|containers?).*(running|failed|ready)",
        r"jakie\s+(pody|nody|kontenery).*(działają|nie działają)",
        r"w\s+namespace",
        r"in\s+(namespace|production|staging|default)",
        r"na\s+nodzie",
        r"on\s+node",
        r"zużycie\s+(cpu|memory|pamięć)",
        r"(cpu|memory|disk)\s+(usage|utilization)",
        r"(restart|błęd|error|failed|crash)",
        r"status\s+(pod|deployment|node)",
        r"\b\d+\b.*(pod|node|replica)",
        r"więcej niż|less than|greater than|ponad",
        r"nginx|mysql|redis|api|backend|frontend|database"
    ]
    
    # Patterns for general knowledge questions
    general_knowledge_patterns = [
        r"czym jest|what is|what are|co to jest",
        r"jak działa|how does.*work|how to",
        r"różnica między|difference between",
        r"explain|wyjaśnij|opisz|describe",
        r"concept|pojęcie|architektura|architecture",
        r"best practices|najlepsze praktyki",
        r"(pod|service|deployment|node|namespace)\b(?!.*\b(in|w)\b.*\b(production|staging|namespace)\b)"
    ]
    
    # Check patterns for cluster status
    for pattern in cluster_status_patterns:
        if re.search(pattern, user_question_lower):
            return QueryType.CLUSTER_STATUS
            
    # Check patterns for general knowledge
    for pattern in general_knowledge_patterns:
        if re.search(pattern, user_question_lower):
            return QueryType.GENERAL_KNOWLEDGE
    
    # Default to treating as cluster status question
    return QueryType.CLUSTER_STATUS
```

## 4. Database Search with Filters

```python
def search_knowledge(self, query: str, query_type: QueryType, limit: int = 5) -> List[Dict[Any, Any]]:
    """Searches for relevant information from the database based on the question type"""
    collection = self.client.collections.get(self.collection_name)
    
    from weaviate.classes.query import Filter

    # Execute vector search with filter
    response = collection.query.near_text(
        query=query,
        limit=limit,
        # Select appropriate filter based on question type
        filters=(
            Filter.by_property("source").equal("markdown") 
            if query_type == QueryType.GENERAL_KNOWLEDGE 
            else Filter.by_property("source").equal("csv")
        ),
        return_properties=["text", "source", "chunk_type", "category", 
                         "section", "subsection", "query", "metadata_json"]
    )
    
    # Process results
    results = []
    for obj in response.objects:
        result = obj.properties.copy()
        # Parse metadata from JSON string
        try:
            result["metadata"] = json.loads(result["metadata_json"])
        except:
            result["metadata"] = {}
        del result["metadata_json"]
        results.append(result)
        
    return results
```

## 5. LLM Integration (Ollama)

```python
class KubernetesLLMIntegration:
    """Class integrating the RAG system with an LLM (Ollama)"""
    
    def __init__(self, weaviate_url: str = "localhost", port: int = 8080):
        self.rag_system = KubernetesRAGSystem(weaviate_url, port)
        # Configuration for local Ollama
        self.openai_api_key = os.getenv("OPENAI_API_KEY") or ""

def generate_response(self, user_question: str) -> Dict[str, Any]:
    """Generates an LLM response based on context from Weaviate"""
    # Get context from Weaviate
    context = self.rag_system.get_llm_context(user_question)
    
    # Prepare prompt for LLM
    system_prompt = self._build_system_prompt(context)
    
    # Call local Ollama via OpenAI-compatible API
    try:
        from openai import OpenAI
        openai = OpenAI(
            base_url="http://localhost:11434/v1", 
            api_key=self.openai_api_key
        )
        
        response = openai.chat.completions.create(
            model="llama3.1:latest",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_question}
            ],
            temperature=0.1,
            max_tokens=1000
        )

        openai.close()
        
        return {
            "user_question": user_question,
            "llm_response": response.choices[0].message.content,
            "context_used": context,
            "sources_count": len(context["results"]),
            "source_type": context["source_type"]
        }
        
    except Exception as e:
        if 'openai' in locals():
            openai.close()
        return {
            "user_question": user_question,
            "error": f"LLM Error: {str(e)}",
            "context_used": context
        }
```

## 6. System Prompts

```python
def _build_system_prompt(self, context: Dict[str, Any]) -> str:
    """Builds the system prompt based on the question type and context"""
    if context["source_type"] == "markdown":
        prompt = """You are a Kubernetes expert who answers general questions about concepts and architecture.
                    You base your answers on the provided information from the documentation.
                    Context from the knowledge base:
                    """
    else:
        prompt = """You are a Kubernetes monitoring expert who helps create PromQL queries and analyze cluster status.
                    You base your answers on the provided examples of PromQL queries.
                    Context from the knowledge base (PromQL queries):
                    """
    
    # Add results from Weaviate
    for i, result in enumerate(context["results"][:3], 1):  # max 3 results
        prompt += f"\n{i}. Category: {result['category']}"
        prompt += f"\n   Section: {result['section']}"
        if result.get('query'):
            prompt += f"\n   PromQL Query: {result['query']}"
        prompt += f"\n   Description: {result['text'][:300]}...\n"
    
    action_type = ("If the question is about a specific cluster state, provide the appropriate PromQL query" 
                  if context['source_type'] == 'csv' 
                  else 'Explain the concepts clearly and understandably')
    
    prompt += f"""\nYour tasks:
            1. Answer the user's question based ONLY on the provided context
            2. {action_type}
            3. If you do not have enough information, state it clearly
            4. Respond in English
            5. Be precise and specific"""
    
    return prompt
```

## Usage Examples

### General Knowledge Questions (source: markdown)
- "What is a pod in Kubernetes?"
- "How does a Service work in Kubernetes?"
- "What are the best practices for a Deployment?"

### Cluster Status Questions (source: csv)
- "How many pods are running in the production namespace?"
- "Which containers are using the most CPU in production?"
- "Which pods in the default namespace are not ready?"

## Setup Instructions

### 1. System Requirements

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull required models
ollama pull nomic-embed-text
ollama pull llama3.1:latest
```

### 2. Python Dependencies Installation

```bash
pip install weaviate-client python-dotenv
```

### 3. Running Weaviate

```bash
# Run Weaviate with Docker
docker run -p 8080:8080 -p 50051:50051 semitechnologies/weaviate:latest
```

### 4. Environment Variables Configuration

```bash
# Create .env file (optional - application sets empty value)
echo "OPENAI_API_KEY=" > .env
```

### 5. System Initialization

```python
# Run setup_system() - only once at the beginning
def setup_system():
    """Function for system setup - run once at the beginning"""
    print("=== SYSTEM SETUP ===")
    
    # Check if data file exists
    json_file = "promql_chunks.json"
    if not os.path.exists(json_file):
        print(f"ERROR: File not found {json_file}")
        return False
    
    # RAG system initialization
    rag_system = KubernetesRAGSystem()
    
    try:
        # Create collection
        rag_system.setup_collection()
        
        # Import data
        rag_system.import_data(json_file)
        
        print("\n✅ System configured successfully!")
        return True
        
    except Exception as e:
        print(f"❌ Error during setup: {str(e)}")
        rag_system.client.close()
        return False

# Run setup
setup_system()
```

### 6. System Testing

```python
def main():
    """Test function"""
    llm_integration = KubernetesLLMIntegration()
    
    test_questions = [
        "What is a pod in Kubernetes?",  # markdown
        "How many pods are running in the production namespace?",  # csv
        "Which containers are using the most CPU in production?",  # csv
        "How does a Service work in Kubernetes?",  # markdown
    ]
    
    for question in test_questions:
        result = llm_integration.generate_response(question)
        print(f"Question: {question}")
        print(f"Source Type: {result.get('source_type')}")
        print(f"Response: {result.get('llm_response', result.get('error'))}")
        print("-" * 80)

# Run tests
main()
```

## Solution Benefits

1. **Local Infrastructure** - completely independent from external APIs
2. **Automatic Question Classification** - system recognizes question type
3. **Source Filtering** - each question type uses appropriate data
4. **Batch Processing** - efficient data import
5. **Ollama Integration** - utilizes local AI models
6. **Error Handling** - proper error handling and connection closing

## Docker Configuration for Weaviate + Ollama

```yaml
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.4
    ports:
    - 8080:8080
    - 50052:50051
    volumes:
    - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_API_BASED_MODULES: 'true'
      ENABLE_MODULES: 'text2vec-ollama,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'
volumes:
  weaviate_data:
```

## Extensions

- **Add Cache** - implement cache for frequent queries
- **Monitoring** - add logging and performance metrics
- **API Endpoint** - create REST API for integration
- **More Patterns** - extend classification for specialized questions
- **Streaming Responses** - implement streaming response functionality